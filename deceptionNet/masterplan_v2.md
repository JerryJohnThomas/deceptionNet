much tighter masterplan.md


perfect â€” hereâ€™s your **tight high-level TODO list for Stage 2**, distilled straight from that north-star plan, cleanly grouped and sequenced so Codex can expand it later.

---

## ğŸš€ Stage 2 High-Level TODO

### ğŸ¯ 0. North-Star Goals

* Maximize **win-rate** within token/time budget.
* Keep **reliability** (no invalid / stalled moves).
* Ensure **paper novelty** (replicable improvements, not just â€œbigger LLMâ€).

---

### ğŸ§  1. Architecture Upgrades

* [ ] Implement **LLM Listener v2**

  * Faster summarization + caching.
  * Add contradiction/bandwagon detection flags.
* [ ] Implement **Presenter v2**

  * Justification sentences (short rationale per vote/talk).
  * Strict regex guard for invalid or leaking text.

---

### ğŸ§© 2. Training Pipeline

#### (a) Imitation Learning Refresh

* [ ] Expand dataset â†’ ~1 000 synthetic samples (balanced by role & phase).
* [ ] Include contradiction / vote-switch / bandwagon scenes.
* [ ] Train 5â€“6 epochs â†’ `weights-il-v6.pt`.

#### (b) PPO Fine-Tune

* [ ] Warm-start from `il-v6`.
* [ ] Run self-play + bot pool, alternating roles.
* [ ] Rewards:

  * +1 win / â€“1 loss
  * +0.1 legal move
  * +0.05 vote aligned with suspicion
  * â€“0.05 for inconsistent talk
* [ ] ~0.5â€“1.5 M steps â†’ `weights-rl-v1.pt`.

#### (c) Consistency Regularizer

* [ ] Add small auxiliary loss penalizing voteâ€“belief mismatch.
* [ ] Log correlation metric for paper figure.

---

### ğŸ§¬ 3. Novelty Modules (pick 1â€“2)

* [ ] **Counterfactual Listener Update** â†’ simulate alternate claims, compute stability.
* [ ] **Role-Privacy Auditor** â†’ detect hidden-info leaks; measure leak vs win rate.
* [ ] **Calibration-Guided Voting** â†’ add calibration head; enforce agreement between suspicion rank & vote.

---

### âš™ï¸ 4. Inference & Reliability

* [ ] Quantize LLM (â‰¤ 3 B params, â‰¤ 50 ms per call).
* [ ] Cache listener summaries per round.
* [ ] Presenter: enforce â‰¤ 32 tokens + valid â€œ[i]â€ formats.
* [ ] Timeout guard (1.5 s â†’ safe fallback).
* [ ] Add per-game JSON logging (phase, role, intent, target, latency).

---

### ğŸ•“ 5. Execution Timeline

**Today â†’ Tomorrow**

* Implement Listener v2 & Presenter v2.
* Generate 1 k IL v6 dataset; train.
* Optional PPO run â†’ `rl-v1`.

**Oct 24â€“26 (Competition Window)**

* Play â‰¥ 50 games (target 70â€“100).
* Use same model token/identity.
* Pause 10â€“15 s between matches.

**After Window**

* Write report with:

  * IL / PPO loss curves
  * Win rate by role
  * Ablations (LLM listener on/off, reg on/off)

---

### âš–ï¸ 6. Training Knobs (Defaults)

| Parameter | IL   | PPO    |
| --------- | ---- | ------ |
| epochs    | 6    | â€”      |
| batch     | 8    | 64â€“128 |
| lr        | 3e-4 | 3e-5   |
| Î³         | â€”    | 0.99   |
| clip      | â€”    | 0.2    |
| vf coef   | â€”    | 0.5    |
| ent coef  | â€”    | 0.01   |
| Î»         | â€”    | 0.95   |

---

### ğŸ§¯ 7. Fallback Plan

* If LLM unstable â†’ ship `il-v6` only + consistency regularizer + richer template bank.

---

Would you like me to now break this into a **phase-by-phase actionable list** (files + function touchpoints) so Codex can directly start patching each part?
